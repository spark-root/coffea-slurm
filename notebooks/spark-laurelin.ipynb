{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://swan.web.cern.ch/sites/swan.web.cern.ch/files/pictures/logo_swan_letters.png\" alt=\"SWAN\" style=\"float: left; width: 20%; margin-right: 15%; margin-left: 15%; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://spark.apache.org/images/spark-logo-trademark.png\" alt=\"EP-SFT\" style=\"float: left; width: 25%; margin-right: 15%; margin-bottom: 2.0em;\">\n",
    "<p style=\"clear: both;\">\n",
    "<div style=\"text-align:center\"><h1>Physics analysis with Apache Spark using Coffea and Laurelin packages from Fermilab</h1></div>\n",
    "<div style=\"text-align:center\"><i>Author: Lindsey Gray; Contact: Lindsey Gray / Prasanth Kothuri</i></div>\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimuon spectrum\n",
    "\n",
    "This code is a columnar adaptation of [a ROOT tutorial](https://root.cern.ch/doc/master/df102__NanoAODDimuonAnalysis_8py.html) showcasing the awkward array toolset, and utilizing Coffea [histograms](https://coffeateam.github.io/coffea/modules/coffea.hist.html).\n",
    "This also shows the analysis object syntax implemented by Coffea [JaggedCandidateArray](https://coffeateam.github.io/coffea/api/coffea.analysis_objects.JaggedCandidateMethods.html), and the usage of custom [accumulators](https://coffeateam.github.io/coffea/api/coffea.processor.AccumulatorABC.html) other than histograms.  Further, it introduces the [processor](https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html) concept and the interface to apache spark.\n",
    "\n",
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_EXECUTION_MODE=True\n",
    "LOCAL_FILE_MODE=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Payload\n",
    "Now let's make some Coffea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# All of these configs are either:\n",
    "#   * Coffea specific and rolled into the library\n",
    "#   * Site-specific and rolled into site-wide Spark configs\n",
    "# The goal is that a user never sees any of this\n",
    "#\n",
    "\n",
    "import pyspark\n",
    "import os, os.path\n",
    "\n",
    "# Work around arrow compatibility issue\n",
    "os.environ['ARROW_PRE_0_15_IPC_FORMAT'] = \"1\"\n",
    "\n",
    "if LOCAL_EXECUTION_MODE:\n",
    "    masterURL = \"local[16]\"\n",
    "else:\n",
    "    masterURL = \"spark://localhost:7077\"\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .master(masterURL) \\\n",
    "    .config('spark.jars.packages',\n",
    "            'edu.vanderbilt.accre:laurelin:1.0.2') \\\n",
    "    .config('spark.driver.memory', '24g') \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .config('spark.sql.execution.arrow.enabled', 'true') \\\n",
    "    .config('spark.speculation', 'true') \\\n",
    "    .config('spark.sql.execution.arrow.maxRecordsPerBatch', 200000) \\\n",
    "    .config('spark.driver.extraClassPath', os.path.join(os.getcwd(), '..', 'hadoop-xrootd-1.0.4-jar-with-dependencies.jar')) \\\n",
    "    .config('spark.executor.extraClassPath', os.path.join(os.getcwd(), '..', 'hadoop-xrootd-1.0.4-jar-with-dependencies.jar')) \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "import coffea.processor as processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at ProcessorABC documentation to see the expected methods and what they are supposed to do\n",
    "class DimuonProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        self._columns = ['nMuon', 'Muon_pt', 'Muon_eta', 'Muon_phi', 'Muon_mass', 'Muon_charge']\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"Primary dataset\")\n",
    "        mass_axis = hist.Bin(\"mass\", r\"$m_{\\mu\\mu}$ [GeV]\", 30000, 0.25, 300)\n",
    "        \n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'mass': hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            'cutflow': processor.defaultdict_accumulator(int),\n",
    "        })\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "    \n",
    "    def process(self, df):\n",
    "        output = self.accumulator.identity()\n",
    "        \n",
    "        dataset = df['dataset']\n",
    "        muons = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nMuon'],\n",
    "            pt=df['Muon_pt'].content,\n",
    "            eta=df['Muon_eta'].content,\n",
    "            phi=df['Muon_phi'].content,\n",
    "            mass=df['Muon_mass'].content,\n",
    "            charge=df['Muon_charge'].content,\n",
    "            )\n",
    "        \n",
    "        output['cutflow']['all events'] += muons.size\n",
    "        \n",
    "        twomuons = (muons.counts == 2)\n",
    "        output['cutflow']['two muons'] += twomuons.sum()\n",
    "        \n",
    "        opposite_charge = twomuons & (muons['charge'].prod() == -1)\n",
    "        output['cutflow']['opposite charge'] += opposite_charge.sum()\n",
    "        \n",
    "        dimuons = muons[opposite_charge].distincts()\n",
    "        output['mass'].fill(dataset=dataset, mass=dimuons.mass.flatten())\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "from pyarrow.compat import guid\n",
    "from coffea.processor.spark.detail import _spark_initialize, _spark_stop\n",
    "from coffea.processor.spark.spark_executor import spark_executor\n",
    "\n",
    "print(\"PWD IS %s \" % os.getcwd())\n",
    "\n",
    "partitionsize = 200000\n",
    "thread_workers = 16\n",
    "\n",
    "tstart = time.time()    \n",
    "if LOCAL_FILE_MODE == False:\n",
    "    file_prefix = 'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/'\n",
    "else:\n",
    "    file_prefix = '../input/'\n",
    "    \n",
    "fileset = {\n",
    "    'DoubleMuon': { 'files': [\n",
    "        file_prefix + 'Run2012B_DoubleMuParked.root',\n",
    "        file_prefix + 'Run2012C_DoubleMuParked.root',\n",
    "                             ], \n",
    "                    'treename': 'Events'\n",
    "                  }\n",
    "}\n",
    "\n",
    "output = processor.run_spark_job(fileset, DimuonProcessor(), spark_executor, \n",
    "                                 spark=spark, partitionsize=partitionsize, thread_workers=thread_workers,\n",
    "                                 executor_args={'file_type': 'edu.vanderbilt.accre.laurelin.Root', 'cache': False}\n",
    "                                )\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = hist.plot1d(output['mass'], overlay='dataset')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(0.1, 1e6)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ax.figure.set_size_inches(10,10)\n",
    "txt_opts = {'horizontalalignment': 'center',\n",
    "            'verticalalignment': 'center',\n",
    "            'transform': ax.transAxes}\n",
    "plt.text(0.85, 0.75, 'Z', **txt_opts)\n",
    "plt.text(0.55, 0.77, r\"$\\Upsilon$(1,2,3S)\", **txt_opts)\n",
    "plt.text(0.37, 0.95, r\"J/$\\Psi$\", **txt_opts)\n",
    "plt.text(0.40, 0.77, r\"$\\Psi$'\", **txt_opts)\n",
    "plt.text(0.22, 0.80, r\"$\\phi$\", **txt_opts)\n",
    "plt.text(0.16, 0.83, r\"$\\rho,\\omega$\", **txt_opts)\n",
    "plt.text(0.11, 0.78, r\"$\\eta$\", **txt_opts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Events/s:\", output['cutflow']['all events']/elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "sparkconnect": {
   "bundled_options": [
    "MemoryIntensive"
   ],
   "list_of_options": [
    {
     "name": "spark.kubernetes.container.image",
     "value": "gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin"
    },
    {
     "name": "spark.sql.execution.arrow.enabled",
     "value": "true"
    },
    {
     "name": "spark.sql.execution.arrow.maxRecordsPerBatch",
     "value": "200000"
    },
    {
     "name": "spark.kubernetes.container.image.pullPolicy",
     "value": "Always"
    },
    {
     "name": "spark.driver.extraClassPath",
     "value": "./laurelin-0.5.1.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
